{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../..\")\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "from transport_nets.bijectors import BananaMap, BananaFlow\n",
    "tfd = tfp.distributions\n",
    "tfb = tfp.bijectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_disc_model(nn_list):\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(layers.Dense(nn_list[0]))\n",
    "    model.add(layers.LeakyReLU())\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.Dense(nn_list[1]))\n",
    "    model.add(layers.LeakyReLU())\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.Dense(nn_list[2]))\n",
    "    model.add(layers.LeakyReLU())\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.Dense(1,activation='sigmoid'))\n",
    "    \n",
    "    return model\n",
    "\n",
    "def make_gen_model(nn_list,output_dim):\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(layers.Dense(nn_list[0]))\n",
    "    model.add(layers.LeakyReLU())\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.Dense(nn_list[1]))\n",
    "    model.add(layers.LeakyReLU())\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.Dense(nn_list[2]))\n",
    "    model.add(layers.LeakyReLU())\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.Dense(output_dim))\n",
    "\n",
    "    return model\n",
    "\n",
    "class T_gen(tf.keras.Model):\n",
    "    \n",
    "    def __init__(self,n,m,K_nn_list,F_nn_list):\n",
    "        super(T_gen,self).__init__(name='T')\n",
    "        self.n = n\n",
    "        self.m = m\n",
    "        self.K = make_gen_model(K_nn_list,n)\n",
    "        self.F = make_gen_model(F_nn_list,m)\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        x = inputs[...,:self.n]\n",
    "        y = inputs[...,self.n:]\n",
    "        T1 = self.K(x)\n",
    "        T2 = self.F(tf.concat([T1,y],axis=-1))\n",
    "        \n",
    "        return tf.concat([T1,T2],axis = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "it: 0 D loss: [1.3806748] G loss: [0.6815028]\n",
      "it: 10 D loss: [1.3881774] G loss: [0.68130857]\n",
      "it: 20 D loss: [1.3768718] G loss: [0.7296488]\n",
      "it: 30 D loss: [1.3941169] G loss: [0.6778769]\n",
      "it: 40 D loss: [1.3665285] G loss: [0.6577923]\n",
      "it: 50 D loss: [1.3927486] G loss: [0.6815985]\n",
      "it: 60 D loss: [1.3810556] G loss: [0.71837795]\n",
      "it: 70 D loss: [1.3692669] G loss: [0.7322038]\n",
      "it: 80 D loss: [1.3926544] G loss: [0.69731385]\n",
      "it: 90 D loss: [1.3827499] G loss: [0.69271773]\n",
      "it: 100 D loss: [1.3835373] G loss: [0.6985604]\n",
      "it: 110 D loss: [1.3901974] G loss: [0.68363625]\n",
      "it: 120 D loss: [1.38146] G loss: [0.69118667]\n",
      "it: 130 D loss: [1.3858037] G loss: [0.696331]\n",
      "it: 140 D loss: [1.3853552] G loss: [0.68967885]\n",
      "it: 150 D loss: [1.384937] G loss: [0.68377745]\n",
      "it: 160 D loss: [1.3874123] G loss: [0.679668]\n",
      "it: 170 D loss: [1.3865869] G loss: [0.6736726]\n",
      "it: 180 D loss: [1.3860171] G loss: [0.6796594]\n",
      "it: 190 D loss: [1.3813926] G loss: [0.69246066]\n",
      "it: 200 D loss: [1.373996] G loss: [0.7000928]\n",
      "it: 210 D loss: [1.3800442] G loss: [0.6924736]\n",
      "it: 220 D loss: [1.3853815] G loss: [0.68910825]\n",
      "it: 230 D loss: [1.3813521] G loss: [0.68820274]\n",
      "it: 240 D loss: [1.3866415] G loss: [0.68583655]\n",
      "it: 250 D loss: [1.3898277] G loss: [0.69057596]\n",
      "it: 260 D loss: [1.3814414] G loss: [0.6804768]\n",
      "it: 270 D loss: [1.3868115] G loss: [0.7036489]\n",
      "it: 280 D loss: [1.3846567] G loss: [0.6809722]\n",
      "it: 290 D loss: [1.386152] G loss: [0.6801982]\n",
      "it: 300 D loss: [1.3828173] G loss: [0.6808035]\n",
      "it: 310 D loss: [1.3838971] G loss: [0.677767]\n",
      "it: 320 D loss: [1.3866992] G loss: [0.68017536]\n",
      "it: 330 D loss: [1.3870757] G loss: [0.6837493]\n",
      "it: 340 D loss: [1.3873117] G loss: [0.6785145]\n",
      "it: 350 D loss: [1.3912668] G loss: [0.67982537]\n",
      "it: 360 D loss: [1.3826587] G loss: [0.6891664]\n",
      "it: 370 D loss: [1.380492] G loss: [0.6887136]\n",
      "it: 380 D loss: [1.3887787] G loss: [0.69337]\n",
      "it: 390 D loss: [1.3733938] G loss: [0.70002]\n",
      "it: 400 D loss: [1.383231] G loss: [0.68254155]\n",
      "it: 410 D loss: [1.3801898] G loss: [0.6820145]\n",
      "it: 420 D loss: [1.3903973] G loss: [0.6898731]\n",
      "it: 430 D loss: [1.3807317] G loss: [0.6804959]\n",
      "it: 440 D loss: [1.3820317] G loss: [0.68172]\n",
      "it: 450 D loss: [1.3848743] G loss: [0.6776213]\n",
      "it: 460 D loss: [1.3871994] G loss: [0.67683446]\n",
      "it: 470 D loss: [1.3870766] G loss: [0.6755562]\n",
      "it: 480 D loss: [1.383524] G loss: [0.6819621]\n",
      "it: 490 D loss: [1.3876618] G loss: [0.68566537]\n"
     ]
    }
   ],
   "source": [
    "# Using Adam Optimizer for minimizing the Generator and Discriminator loss\n",
    "lr = 1e-5\n",
    "BATCH_SIZE = 100\n",
    "BUFFER_SIZE = 10000\n",
    "n = 1\n",
    "m = 1\n",
    "latent_dim = n+m\n",
    "lamda = 0.01 # monoticity lagrange multiplier \n",
    "disc_nn_list = [200,500,100]\n",
    "K_nn_list = [100,200,100]\n",
    "F_nn_list = [200,500,100]\n",
    "N = 5000\n",
    "N_epochs = 500\n",
    "\n",
    "\n",
    "params = (0.5,0.1,0.05,0.0) #(a1,a2,a3,theta)\n",
    "bMap = BananaMap(params)\n",
    "bFlow = BananaFlow(bMap)\n",
    "\n",
    "XT = bFlow.sample(N)\n",
    "x = XT[...,1:]\n",
    "y = XT[...,:1]\n",
    "train_dataset_tensor = tf.concat([x,y],axis=-1)\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices(train_dataset_tensor).shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
    "\n",
    "f = make_disc_model(disc_nn_list)\n",
    "T = T_gen(n,m,K_nn_list,F_nn_list)\n",
    "\n",
    "gen_opt = tf.keras.optimizers.Adam(learning_rate=lr, epsilon=1e-8)\n",
    "disc_opt = tf.keras.optimizers.Adam(learning_rate=lr, epsilon=1e-8)\n",
    "\n",
    "def gen_loss():\n",
    "    z1 = tf.random.normal([BATCH_SIZE,latent_dim])\n",
    "    z2 = tf.random.normal([BATCH_SIZE,latent_dim])\n",
    "    T1 = T(z1)\n",
    "    T2 = T(z2)\n",
    "    g_loss_fake = tf.reduce_mean(tf.math.log(f(T1)),axis=0)\n",
    "    m_loss = lamda*tf.reduce_mean(tf.reduce_sum((T1-T2)*(z1-z2),axis=1))\n",
    "    g_loss = g_loss_fake + m_loss\n",
    "    return -g_loss\n",
    "\n",
    "def disc_loss(x):\n",
    "    z = tf.random.normal([BATCH_SIZE,latent_dim])\n",
    "    d_loss_real = tf.reduce_mean(tf.math.log(f(x)),axis=0)\n",
    "    d_loss_fake = tf.reduce_mean(tf.math.log(1.0-f(T(z))),axis=0)\n",
    "    d_loss = d_loss_real+d_loss_fake\n",
    "    return -d_loss \n",
    "    \n",
    "\n",
    "# input x is minibatch of data points\n",
    "@tf.function\n",
    "def train_step(x): \n",
    "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
    "        g_loss = gen_loss()\n",
    "        d_loss = disc_loss(x)\n",
    "        \n",
    "    gen_grad = gen_tape.gradient(g_loss, T.trainable_variables)\n",
    "    disc_grad = disc_tape.gradient(d_loss, f.trainable_variables)\n",
    "    \n",
    "    gen_opt.apply_gradients(zip(gen_grad,T.trainable_variables))\n",
    "    disc_opt.apply_gradients(zip(disc_grad,f.trainable_variables))\n",
    "    \n",
    "    return d_loss,g_loss\n",
    "\n",
    "def train(dataset, epochs):\n",
    "    for epoch in range(epochs):\n",
    "        for data_batch in dataset:\n",
    "            d_loss,g_loss = train_step(data_batch)\n",
    "        if epoch % 10 == 0: \n",
    "            print('it:',epoch,\"D loss:\",d_loss.numpy(),\"G loss:\",g_loss.numpy())\n",
    "            \n",
    "train(train_dataset,N_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "def plot_density(data,axis):\n",
    "    x, y = np.squeeze(np.split(data, 2, axis=1))\n",
    "    return sns.kdeplot(x, y, cmap=\"viridis\", shade=True, \n",
    "                     shade_lowest=True, ax=axis)\n",
    "xa,xb,ya,yb = (-1.5,1.5,-0.2,0.6)\n",
    "fig,ax = plt.subplots(1,2,figsize=(10,4))\n",
    "X = bFlow.sample(5000)\n",
    "X_p = T(tf.random.normal([5000,2]))\n",
    "flip = tfb.Permute([1,0])\n",
    "X_flipped = flip.forward(X_p)\n",
    "l1 = plot_density(X,axis=ax[0])\n",
    "l2 = plot_density(X_flipped,axis=ax[1])\n",
    "ax[0].set(xlim=(xa,xb),ylim=(ya,yb))\n",
    "ax[1].set(xlim=(xa,xb),ylim=(ya,yb))\n",
    "\n",
    "ax[0].set_title('true map')\n",
    "ax[1].set_title('MGAN map')\n",
    "plt.savefig('MGAN_kde.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2,2,figsize=(8,6))\n",
    "x_obs_list = [0.5,0.2,0.0]\n",
    "ax[0,0].scatter(X[:,0],X[:,1],alpha=0.2,label='true data')\n",
    "ax[0,0].scatter(X_flipped[:,0],X_flipped[:,1],alpha=0.2,label='MGAN samples')\n",
    "ax[0,0].plot(np.linspace(-2,2,100),x_obs_list[0]*np.ones(100),'r--')\n",
    "ax[0,0].plot(np.linspace(-2,2,100),x_obs_list[1]*np.ones(100),'g--')\n",
    "ax[0,0].plot(np.linspace(-2,2,100),x_obs_list[2]*np.ones(100),'--',c='purple')\n",
    "ax[0,0].set(xlabel='y',ylabel='x')\n",
    "ax[0,0].legend()\n",
    "\n",
    "Ns = 2000\n",
    "u = tf.random.normal([Ns,1])\n",
    "\n",
    "x_obs1 = x_obs_list[0]*tf.ones([Ns,1])\n",
    "x_obs2 = x_obs_list[1]*tf.ones([Ns,1])\n",
    "x_obs3 = x_obs_list[2]*tf.ones([Ns,1])\n",
    "y_sample1 = T.F(tf.concat([x_obs1,u],axis=-1))[...,0]\n",
    "y_sample2 = T.F(tf.concat([x_obs2,u],axis=-1))[...,0]\n",
    "y_sample3 = T.F(tf.concat([x_obs3,u],axis=-1))[...,0]\n",
    "ax[0,1].hist(y_sample1,40,color='r',density=True)\n",
    "ax[1,0].hist(y_sample2,40,color='g',density=True)\n",
    "ax[1,1].hist(y_sample3,40,color='purple',density=True)\n",
    "plt.tight_layout()\n",
    "plt.savefig('MGAN_conditional_samples.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"T\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "sequential_1 (Sequential)    multiple                  42201     \n",
      "_________________________________________________________________\n",
      "sequential_2 (Sequential)    multiple                  154501    \n",
      "=================================================================\n",
      "Total params: 196,702\n",
      "Trainable params: 194,302\n",
      "Non-trainable params: 2,400\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "T.summary()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
