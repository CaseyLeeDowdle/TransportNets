{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../..\")\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "from transport_nets.distributions import BananaFlow\n",
    "tfd = tfp.distributions\n",
    "tfb = tfp.bijectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_disc_model(nn_list):\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(layers.Dense(nn_list[0]))\n",
    "    model.add(layers.LeakyReLU())\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.Dense(nn_list[1]))\n",
    "    model.add(layers.LeakyReLU())\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.Dense(nn_list[2]))\n",
    "    model.add(layers.LeakyReLU())\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.Dense(1,activation='sigmoid'))\n",
    "    \n",
    "    return model\n",
    "\n",
    "def make_gen_model(nn_list,output_dim):\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(layers.Dense(nn_list[0]))\n",
    "    model.add(layers.LeakyReLU())\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.Dense(nn_list[1]))\n",
    "    model.add(layers.LeakyReLU())\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.Dense(nn_list[2]))\n",
    "    model.add(layers.LeakyReLU())\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.Dense(output_dim))\n",
    "\n",
    "    return model\n",
    "\n",
    "class T_gen(tf.keras.Model):\n",
    "    \n",
    "    def __init__(self,n,m,K_nn_list,F_nn_list):\n",
    "        super(T_gen,self).__init__(name='T')\n",
    "        self.n = n\n",
    "        self.m = m\n",
    "        self.K = make_gen_model(K_nn_list,n)\n",
    "        self.F = make_gen_model(F_nn_list,m)\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        x = inputs[...,:self.n]\n",
    "        y = inputs[...,self.n:]\n",
    "        T1 = self.K(x)\n",
    "        T2 = self.F(tf.concat([T1,y],axis=-1))\n",
    "        \n",
    "        return tf.concat([T1,T2],axis = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "it: 0 D loss: [1.3681016] G loss: [0.6874191]\n",
      "it: 10 D loss: [1.3860567] G loss: [0.6736822]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-2cf1cfb7397d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'it:'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"D loss:\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0md_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"G loss:\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mg_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mN_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-3-2cf1cfb7397d>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(dataset, epochs)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mdata_batch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m             \u001b[0md_loss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mg_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m10\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'it:'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"D loss:\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0md_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"G loss:\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mg_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    578\u001b[0m         \u001b[0mxla_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 580\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    581\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    582\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    609\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    610\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 611\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    612\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    613\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2418\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2419\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2420\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2422\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   1663\u001b[0m          if isinstance(t, (ops.Tensor,\n\u001b[1;32m   1664\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[0;32m-> 1665\u001b[0;31m         self.captured_inputs)\n\u001b[0m\u001b[1;32m   1666\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1667\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1744\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1745\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1746\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1747\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1748\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    596\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    597\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 598\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    599\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    600\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Using Adam Optimizer for minimizing the Generator and Discriminator loss\n",
    "lr = 1e-5\n",
    "BATCH_SIZE = 100\n",
    "BUFFER_SIZE = 10000\n",
    "n = 1\n",
    "m = 1\n",
    "latent_dim = n+m\n",
    "lamda = 0.01 # monoticity lagrange multiplier \n",
    "disc_nn_list = [200,500,100]\n",
    "K_nn_list = [100,200,100]\n",
    "F_nn_list = [200,500,100]\n",
    "N = 5000\n",
    "N_epochs = 500\n",
    "\n",
    "\n",
    "params = (0.5,0.1,0.05,0.0) #(a1,a2,a3,theta)\n",
    "bFlow = BananaFlow(params)\n",
    "\n",
    "XT = bFlow.sample(N)\n",
    "x = XT[...,1:]\n",
    "y = XT[...,:1]\n",
    "train_dataset_tensor = tf.concat([x,y],axis=-1)\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices(train_dataset_tensor).shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
    "\n",
    "f = make_disc_model(disc_nn_list)\n",
    "T = T_gen(n,m,K_nn_list,F_nn_list)\n",
    "\n",
    "gen_opt = tf.keras.optimizers.Adam(learning_rate=lr, epsilon=1e-8)\n",
    "disc_opt = tf.keras.optimizers.Adam(learning_rate=lr, epsilon=1e-8)\n",
    "\n",
    "def gen_loss():\n",
    "    z1 = tf.random.normal([BATCH_SIZE,latent_dim])\n",
    "    z2 = tf.random.normal([BATCH_SIZE,latent_dim])\n",
    "    T1 = T(z1)\n",
    "    T2 = T(z2)\n",
    "    g_loss_fake = tf.reduce_mean(tf.math.log(f(T1)),axis=0)\n",
    "    m_loss = lamda*tf.reduce_mean(tf.reduce_sum((T1-T2)*(z1-z2),axis=1))\n",
    "    g_loss = g_loss_fake + m_loss\n",
    "    return -g_loss\n",
    "\n",
    "def disc_loss(x):\n",
    "    z = tf.random.normal([BATCH_SIZE,latent_dim])\n",
    "    d_loss_real = tf.reduce_mean(tf.math.log(f(x)),axis=0)\n",
    "    d_loss_fake = tf.reduce_mean(tf.math.log(1.0-f(T(z))),axis=0)\n",
    "    d_loss = d_loss_real+d_loss_fake\n",
    "    return -d_loss \n",
    "    \n",
    "\n",
    "# input x is minibatch of data points\n",
    "@tf.function\n",
    "def train_step(x): \n",
    "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
    "        g_loss = gen_loss()\n",
    "        d_loss = disc_loss(x)\n",
    "        \n",
    "    gen_grad = gen_tape.gradient(g_loss, T.trainable_variables)\n",
    "    disc_grad = disc_tape.gradient(d_loss, f.trainable_variables)\n",
    "    \n",
    "    gen_opt.apply_gradients(zip(gen_grad,T.trainable_variables))\n",
    "    disc_opt.apply_gradients(zip(disc_grad,f.trainable_variables))\n",
    "    \n",
    "    return d_loss,g_loss\n",
    "\n",
    "def train(dataset, epochs):\n",
    "    for epoch in range(epochs):\n",
    "        for data_batch in dataset:\n",
    "            d_loss,g_loss = train_step(data_batch)\n",
    "        if epoch % 10 == 0: \n",
    "            print('it:',epoch,\"D loss:\",d_loss.numpy(),\"G loss:\",g_loss.numpy())\n",
    "            \n",
    "train(train_dataset,N_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "def plot_density(data,axis):\n",
    "    x, y = np.squeeze(np.split(data, 2, axis=1))\n",
    "    return sns.kdeplot(x, y, cmap=\"viridis\", shade=True, \n",
    "                     shade_lowest=True, ax=axis)\n",
    "xa,xb,ya,yb = (-1.5,1.5,-0.2,0.6)\n",
    "fig,ax = plt.subplots(1,2,figsize=(10,4))\n",
    "X = bFlow.sample(5000)\n",
    "X_p = T(tf.random.normal([5000,2]))\n",
    "flip = tfb.Permute([1,0])\n",
    "X_flipped = flip.forward(X_p)\n",
    "l1 = plot_density(X,axis=ax[0])\n",
    "l2 = plot_density(X_flipped,axis=ax[1])\n",
    "ax[0].set(xlim=(xa,xb),ylim=(ya,yb))\n",
    "ax[1].set(xlim=(xa,xb),ylim=(ya,yb))\n",
    "\n",
    "ax[0].set_title('true map')\n",
    "ax[1].set_title('MGAN map')\n",
    "plt.savefig('MGAN_kde.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2,2,figsize=(8,6))\n",
    "x_obs_list = [0.5,0.2,0.0]\n",
    "ax[0,0].scatter(X[:,0],X[:,1],alpha=0.2,label='true data')\n",
    "ax[0,0].scatter(X_flipped[:,0],X_flipped[:,1],alpha=0.2,label='MGAN samples')\n",
    "ax[0,0].plot(np.linspace(-2,2,100),x_obs_list[0]*np.ones(100),'r--')\n",
    "ax[0,0].plot(np.linspace(-2,2,100),x_obs_list[1]*np.ones(100),'g--')\n",
    "ax[0,0].plot(np.linspace(-2,2,100),x_obs_list[2]*np.ones(100),'--',c='purple')\n",
    "ax[0,0].set(xlabel='y',ylabel='x')\n",
    "ax[0,0].legend()\n",
    "\n",
    "Ns = 2000\n",
    "u = tf.random.normal([Ns,1])\n",
    "\n",
    "x_obs1 = x_obs_list[0]*tf.ones([Ns,1])\n",
    "x_obs2 = x_obs_list[1]*tf.ones([Ns,1])\n",
    "x_obs3 = x_obs_list[2]*tf.ones([Ns,1])\n",
    "y_sample1 = T.F(tf.concat([x_obs1,u],axis=-1))[...,0]\n",
    "y_sample2 = T.F(tf.concat([x_obs2,u],axis=-1))[...,0]\n",
    "y_sample3 = T.F(tf.concat([x_obs3,u],axis=-1))[...,0]\n",
    "ax[0,1].hist(y_sample1,40,color='r',density=True)\n",
    "ax[1,0].hist(y_sample2,40,color='g',density=True)\n",
    "ax[1,1].hist(y_sample3,40,color='purple',density=True)\n",
    "plt.tight_layout()\n",
    "plt.savefig('MGAN_conditional_samples.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"T\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "sequential_1 (Sequential)    multiple                  42201     \n",
      "_________________________________________________________________\n",
      "sequential_2 (Sequential)    multiple                  154501    \n",
      "=================================================================\n",
      "Total params: 196,702\n",
      "Trainable params: 194,302\n",
      "Non-trainable params: 2,400\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "T.summary()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
